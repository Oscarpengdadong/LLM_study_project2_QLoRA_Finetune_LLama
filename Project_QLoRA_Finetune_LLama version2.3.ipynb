{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f7687b13cde4ebda6c2a9ae5dc6efa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6823064908e349edaecf2336f97d43bd",
              "IPY_MODEL_ddf6217e2b564ff1ad3f9a9bc7fb8907",
              "IPY_MODEL_28e802940ee64b0eb68ff5ab6da48fe4"
            ],
            "layout": "IPY_MODEL_cd87bd9a619d4afcbcdaa67fa3a1facb"
          }
        },
        "6823064908e349edaecf2336f97d43bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1145d130ca4b4ee29598c7e69c8a7cba",
            "placeholder": "​",
            "style": "IPY_MODEL_7fe1f1f25ae0435b9a92c7f29a763899",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ddf6217e2b564ff1ad3f9a9bc7fb8907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f64b50b9014b4bfea844170413257a28",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ac44b655b384ae79e19622ac83c9c85",
            "value": 4
          }
        },
        "28e802940ee64b0eb68ff5ab6da48fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82213b6f5c7a4affa5266562b1a9ff2a",
            "placeholder": "​",
            "style": "IPY_MODEL_f8e2d2c91f744b07b863e47d357cfa98",
            "value": " 4/4 [00:18&lt;00:00,  3.79s/it]"
          }
        },
        "cd87bd9a619d4afcbcdaa67fa3a1facb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1145d130ca4b4ee29598c7e69c8a7cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe1f1f25ae0435b9a92c7f29a763899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f64b50b9014b4bfea844170413257a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac44b655b384ae79e19622ac83c9c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82213b6f5c7a4affa5266562b1a9ff2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e2d2c91f744b07b863e47d357cfa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0db10f69454a16aa42e75d33889925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89772fffda604ef8affcc6828fac60be",
              "IPY_MODEL_975d1d759b734b7ea8f0bd4302eecb43",
              "IPY_MODEL_3bd8c6ae39c746909bea9da99f8f6c11"
            ],
            "layout": "IPY_MODEL_5fcb49d961ee4861801739988992e093"
          }
        },
        "89772fffda604ef8affcc6828fac60be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ed9ef5043d7480a9f4a875d55ad1bca",
            "placeholder": "​",
            "style": "IPY_MODEL_63ddd9260b594edfaa71ff522efada09",
            "value": "Map: 100%"
          }
        },
        "975d1d759b734b7ea8f0bd4302eecb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8bc4a25c4c4ee98d9e06c062930a5e",
            "max": 499,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b65472c410f4c40a891886960d15fb9",
            "value": 499
          }
        },
        "3bd8c6ae39c746909bea9da99f8f6c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d9b9e14f1ef479c9d3fb1c7d26856fd",
            "placeholder": "​",
            "style": "IPY_MODEL_f215345316824ba19b4650fb08194557",
            "value": " 499/499 [00:00&lt;00:00, 4698.08 examples/s]"
          }
        },
        "5fcb49d961ee4861801739988992e093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ed9ef5043d7480a9f4a875d55ad1bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ddd9260b594edfaa71ff522efada09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f8bc4a25c4c4ee98d9e06c062930a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b65472c410f4c40a891886960d15fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d9b9e14f1ef479c9d3fb1c7d26856fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f215345316824ba19b4650fb08194557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0238534a1f90498cba9a4ae1faf706f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec42b4c030845f1b9a55a525353c927",
              "IPY_MODEL_8b54aab0a0dd4f959c632987124a1f68",
              "IPY_MODEL_52b2c49314ee4f6c8c126b67c5f2244d"
            ],
            "layout": "IPY_MODEL_d5d6c85742b441269b775c6fdb7f46c3"
          }
        },
        "aec42b4c030845f1b9a55a525353c927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31ed349d888b4be49bc7a991142939f7",
            "placeholder": "​",
            "style": "IPY_MODEL_93168488c84246c3ba669c235951a141",
            "value": "Filter: 100%"
          }
        },
        "8b54aab0a0dd4f959c632987124a1f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9bbc068f2f14f28a5c9fdb042401794",
            "max": 499,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25f46aa1ed7b49cdb525be4220d887e6",
            "value": 499
          }
        },
        "52b2c49314ee4f6c8c126b67c5f2244d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5381570c1fd348b29dced283fe79ba6d",
            "placeholder": "​",
            "style": "IPY_MODEL_4563f568bb784b59a42103e36c035b7f",
            "value": " 499/499 [00:00&lt;00:00, 3845.96 examples/s]"
          }
        },
        "d5d6c85742b441269b775c6fdb7f46c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31ed349d888b4be49bc7a991142939f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93168488c84246c3ba669c235951a141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9bbc068f2f14f28a5c9fdb042401794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f46aa1ed7b49cdb525be4220d887e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5381570c1fd348b29dced283fe79ba6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4563f568bb784b59a42103e36c035b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7744a51826824d62b356bb6925fe5a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0a64a28516c446eb12ab3dbc7b3401c",
              "IPY_MODEL_fb6e7952fe0245e7b6328a1cf45ff8b1",
              "IPY_MODEL_1aaed0ac728248e68d313abcbf469a20"
            ],
            "layout": "IPY_MODEL_2c92f38ecaa545ba9d4ec1a18810e9e4"
          }
        },
        "b0a64a28516c446eb12ab3dbc7b3401c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe6d735f25484e7e89c3f84957d4a131",
            "placeholder": "​",
            "style": "IPY_MODEL_7312e177d462458da32297e589ac02a3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fb6e7952fe0245e7b6328a1cf45ff8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb025d6e45448a997532bffe30cb07c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e6f76f3a38744be92840f482da201af",
            "value": 4
          }
        },
        "1aaed0ac728248e68d313abcbf469a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2edb33ae9ae4ad08f21f72e77ba47f9",
            "placeholder": "​",
            "style": "IPY_MODEL_339fc358e46c41a3ab68e24b8eb44881",
            "value": " 4/4 [00:16&lt;00:00,  3.53s/it]"
          }
        },
        "2c92f38ecaa545ba9d4ec1a18810e9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6d735f25484e7e89c3f84957d4a131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7312e177d462458da32297e589ac02a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb025d6e45448a997532bffe30cb07c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6f76f3a38744be92840f482da201af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2edb33ae9ae4ad08f21f72e77ba47f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339fc358e46c41a3ab68e24b8eb44881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0372b696f0d442f8bd0f31bc77560ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be6c284be8fd42e987399f47f7707417",
              "IPY_MODEL_a9b5410a0bc84976800841ceaf542228",
              "IPY_MODEL_2eb38f7d0cbe431faf02f5a0ae625226"
            ],
            "layout": "IPY_MODEL_5915c28b56c94a17b8328339dc781d14"
          }
        },
        "be6c284be8fd42e987399f47f7707417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b47eac59527d43b88ce2ab3e0e276474",
            "placeholder": "​",
            "style": "IPY_MODEL_6267339145b244d0bb1352462acae3e8",
            "value": "Downloading builder script: "
          }
        },
        "a9b5410a0bc84976800841ceaf542228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d4b3f2dba94ccda905eb6034eff490",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa5547f4e68f45668808ebfa032e9ca0",
            "value": 1
          }
        },
        "2eb38f7d0cbe431faf02f5a0ae625226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fd9f8e584304415a3f1b8656af87895",
            "placeholder": "​",
            "style": "IPY_MODEL_6f73191bcf844f9d84f0739d9feb7d8d",
            "value": " 6.27k/? [00:00&lt;00:00, 708kB/s]"
          }
        },
        "5915c28b56c94a17b8328339dc781d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b47eac59527d43b88ce2ab3e0e276474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6267339145b244d0bb1352462acae3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01d4b3f2dba94ccda905eb6034eff490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "aa5547f4e68f45668808ebfa032e9ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fd9f8e584304415a3f1b8656af87895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f73191bcf844f9d84f0739d9feb7d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Workflow:\n",
        "\n",
        "##Target:\n",
        "finetune LLama model version: on dataset: specific on area: dialog summary\n",
        "\n",
        "##Dataset:\n",
        "huggingface\n",
        "\n",
        "##Base model:\n",
        "setup Libraries\\\n",
        "Download LLama model\\\n",
        "Base model Config setup(quantization setting step is here)\\\n",
        "import pre-trained model\\\n",
        "setup tokenizer\\\n",
        "\n",
        "\n",
        "\n",
        "##Data preprocess:\n",
        "  1: formatting text and summary to \"prompt+input:text + output:summary\"for model\\\n",
        "  2: tokenization, transfet text to token_ids(with attention_mask)\n",
        "  3: use base model to generate 'raw' summary for feeling.\n",
        "\n",
        "##Finetune process:\n",
        "  1:trainning setup:\\\n",
        "    1.1: Set up QLora Moudle(actually LoraConfig, base parameters W parameters NF4 setting + quantization is in earlier base model Config)\n",
        "    1.2: Set up peft_trainer\n",
        "    1.3: .train()\n",
        "\n",
        "  2:check training process:\\\n",
        "    trainable model parameters\\\n",
        "    all model parameters\\\n",
        "    percentage of trainable model parameters\n",
        "\n",
        "  3: train model\\\n",
        "\n",
        "##post-training:\n",
        "\n",
        "1:upload Peft_model from based model + Lora tuend model\n",
        "\n",
        "2:test its ability of generating dialog summary. Compare its results with base model's.\n",
        "\n",
        "3:compute Rouge scores of base model and fine-tuned model, and check if ft_model has better ability for summerizing.\n",
        "\n",
        "##Important comments:\n",
        "\n",
        "1.I don't have good local GPU, and that's main reason I put this project on colab. One biggest debug issue I constantly have is error of torch.device(None). 我在peft_model.train(cuda)时触发 torch.device(None) 的报错，debug去掉了peft_model.to('cuda') 以及原模型 device—map=‘cuda’ 改成‘auto’ 后没问题了。可能是某些子模块或者参数（比如lora参数）没有放到GPU上，引发这个 “device index None” 的错误。\n",
        "然后改成device_map='auto'，Accelerate就会动为模型的不同子模块分配设备，避免设备不一致带来的问题。\n",
        "也可以参考这个回复https://stackoverflow.com/questions/78008119/huggingface-transformer-train-function-throwing-device-received-an-invalid-com?utm_source=chatgpt.com\n",
        "\n",
        "2.If you want to run this file, pls change hugging face access token with your own, and before this step you need to apply premission from\n",
        "Meta Llama 3 on hugging face. The access token shown in this file is changed by randomly generated one, and you will get login error with it.\n",
        "\n",
        "3.On github you can see modified version of this project. That evaluate generated summaries in more details.\n",
        "\n",
        "4.关于checkpointing 主要作用在训练的 前向／反向传播中的激活保存与梯度重计算；而 use_cache 主要作用在推理阶段的 Key/Value 缓存复用，这俩不是一个阶段的事情，开启Key/Value 缓存复用不会加速训练速度。另外我看到有个帖子讨论这件事，https://discuss.huggingface.co/t/why-is-use-cache-incompatible-with-gradient-checkpointing/18811，我的理解是在训练中，通常是一次处理完整序列，模型并不逐 token 生成、累积 past key values。此时 use_cache=True 会让模型保存／返回 past_key_values，但这些缓存可能会干扰后续梯度计算或被视为“状态”而非激活，从而与 checkpointing 的重新计算逻辑冲突。"
      ],
      "metadata": {
        "id": "KUGXeTEpvbIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_24lEsQrRFY",
        "outputId": "ebcccbc5-a29c-452b-d57e-dd7e8d8a037a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (13.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml) (13.580.82)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip  install accelerate peft bitsandbytes transformers trl pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-PSwRv1jvDKm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdAL4s1VvEsD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rDWO7catvDaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "AutoModelForCausalLM,\n",
        "AutoTokenizer,\n",
        "BitsAndBytesConfig,\n",
        "TrainingArguments,\n",
        "pipeline,\n",
        "logging,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "vo3Mhd3HrnFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff55a24-9805-4b42-81a5-256d83237f62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next cell is upload LLama model name on huggingface, and dialog summary dataset \"neil-code/dialogsum-test\". Print first 10 rows to check data content."
      ],
      "metadata": {
        "id": "aTXKhW9HL1Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = \"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\"  # enter huggingface account token \"hf_xxx\"; 如果留空且模型受限会报权限错误\n",
        "if HF_TOKEN:\n",
        "  login(HF_TOKEN)\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "dataset_name = \"neil-code/dialogsum-test\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "pprint(dataset['train'][:3], width=100, compact=False) # pprint is useful to see output of cell in a block rather than in a line\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfajvP94_2ys",
        "outputId": "ba1331dd-4673-44c2-efa9-8c50597cbcf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dialogue': [\"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n\"\n",
            "              '#Person2#: I found it would be a good idea to get a check-up.\\n'\n",
            "              \"#Person1#: Yes, well, you haven't had one for 5 years. You should have one every \"\n",
            "              'year.\\n'\n",
            "              '#Person2#: I know. I figure as long as there is nothing wrong, why go see the '\n",
            "              'doctor?\\n'\n",
            "              '#Person1#: Well, the best way to avoid serious illnesses is to find out about them '\n",
            "              'early. So try to come at least once a year for your own good.\\n'\n",
            "              '#Person2#: Ok.\\n'\n",
            "              '#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, '\n",
            "              'please. Do you smoke, Mr. Smith?\\n'\n",
            "              '#Person2#: Yes.\\n'\n",
            "              '#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. '\n",
            "              'You really should quit.\\n'\n",
            "              \"#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n\"\n",
            "              \"#Person1#: Well, we have classes and some medications that might help. I'll give \"\n",
            "              'you more information before you leave.\\n'\n",
            "              '#Person2#: Ok, thanks doctor.',\n",
            "              '#Person1#: Hello Mrs. Parker, how have you been?\\n'\n",
            "              '#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his '\n",
            "              'vaccines.\\n'\n",
            "              \"#Person1#: Very well. Let's see, according to his vaccination record, Ricky has \"\n",
            "              'received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is '\n",
            "              'due for Hepatitis A, Chickenpox and Measles shots.\\n'\n",
            "              '#Person2#: What about Rubella and Mumps?\\n'\n",
            "              '#Person1#: Well, I can only give him these for now, and after a couple of weeks I '\n",
            "              'can administer the rest.\\n'\n",
            "              '#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time '\n",
            "              'I got it was maybe fifteen years ago!\\n'\n",
            "              \"#Person1#: We will check our records and I'll have the nurse administer and the \"\n",
            "              \"booster as well. Now, please hold Ricky's arm tight, this may sting a little.\",\n",
            "              '#Person1#: Excuse me, did you see a set of keys?\\n'\n",
            "              '#Person2#: What kind of keys?\\n'\n",
            "              '#Person1#: Five keys and a small foot ornament.\\n'\n",
            "              \"#Person2#: What a shame! I didn't see them.\\n\"\n",
            "              \"#Person1#: Well, can you help me look for it? That's my first time here.\\n\"\n",
            "              \"#Person2#: Sure. It's my pleasure. I'd like to help you look for the missing keys.\\n\"\n",
            "              \"#Person1#: It's very kind of you.\\n\"\n",
            "              \"#Person2#: It's not a big deal.Hey, I found them.\\n\"\n",
            "              \"#Person1#: Oh, thank God! I don't know how to thank you, guys.\\n\"\n",
            "              \"#Person2#: You're welcome.\"],\n",
            " 'id': ['train_0', 'train_1', 'train_2'],\n",
            " 'summary': [\"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every \"\n",
            "             \"year. Hawkins'll give some information about their classes and medications to help \"\n",
            "             'Mr. Smith quit smoking.',\n",
            "             'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives '\n",
            "             'Ricky a vaccine.',\n",
            "             \"#Person1#'s looking for a set of keys and asks for #Person2#'s help to find them.\"],\n",
            " 'topic': ['get a check-up', 'vaccines', 'find keys']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next cell, we build a function to format data into instruction data. instruction data is for prompt use.\n",
        "\n",
        "Notice that this function works on every single 'context combo'(id,dialog,summary,topic) and return a (text) added into the 'context combo'."
      ],
      "metadata": {
        "id": "xNfOG4jIMNz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    :param sample: input data\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"\"\"Instruct: Below is an instruction that describes a task.\n",
        "Write a response that appropriately completes the request.\"\"\"\n",
        "    INSTRUCTION_KEY = \"Input: Please Summarize the below conversation.\"\n",
        "    RESPONSE_KEY = \"Output:\"\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    parts = [part for part in [blurb, instruction, input_context, response] if\n",
        "    part]\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "    return sample\n",
        "\n",
        "print(create_prompt_formats(dataset['train'][0])['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqOgVYtqJT_w",
        "outputId": "e218c6ec-dfbb-4445-b577-aeddd2fbfe4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Instruct: Below is an instruction that describes a task.\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "Input: Please Summarize the below conversation.\n",
            "\n",
            "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
            "#Person2#: I found it would be a good idea to get a check-up.\n",
            "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
            "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
            "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
            "#Person2#: Ok.\n",
            "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
            "#Person2#: Yes.\n",
            "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
            "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
            "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
            "#Person2#: Ok, thanks doctor.\n",
            "\n",
            "Output:\n",
            "Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next cell we need to build QLoRA setting. This is because we will quantlize base weight matrix from bf16 to 4-bit, and decoder it on the fly of each training iteration. So Let's start."
      ],
      "metadata": {
        "id": "kHQHpETh_sBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=compute_dtype,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "GYlQ8UXCAKh5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload base model(here we use meta-llama/Llama-3.1-8B). We also check improvement of the size of model with our quantization_config(NF4 for base model weight parameter matrix)"
      ],
      "metadata": {
        "id": "In7io-gUAhVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\n",
        "model_id,\n",
        "dtype=compute_dtype, #`torch_dtype` is deprecated! Use `dtype` instead!\n",
        "device_map=\"auto\",\n",
        "quantization_config=quant_config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9f7687b13cde4ebda6c2a9ae5dc6efa3",
            "6823064908e349edaecf2336f97d43bd",
            "ddf6217e2b564ff1ad3f9a9bc7fb8907",
            "28e802940ee64b0eb68ff5ab6da48fe4",
            "cd87bd9a619d4afcbcdaa67fa3a1facb",
            "1145d130ca4b4ee29598c7e69c8a7cba",
            "7fe1f1f25ae0435b9a92c7f29a763899",
            "f64b50b9014b4bfea844170413257a28",
            "9ac44b655b384ae79e19622ac83c9c85",
            "82213b6f5c7a4affa5266562b1a9ff2a",
            "f8e2d2c91f744b07b863e47d357cfa98"
          ]
        },
        "id": "jPUmBaaOAkAf",
        "outputId": "d936c279-e3ce-4075-9f27-ae95e6437459"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f7687b13cde4ebda6c2a9ae5dc6efa3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add tokenizer"
      ],
      "metadata": {
        "id": "yE443W1CShDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False,\n",
        "  trust_remote_code=True,\n",
        "  padding_side=\"left\",add_eos_token=True,add_bos_token=True)#left padding to promise in generation process we put non-EOS token to right side\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id #llama has special padding token, just to play safe, so we use eos token to be pading token"
      ],
      "metadata": {
        "id": "elIRDEamSggy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tese our base model before fine-tune: inference：\n",
        "1: build gen() function to create template\n",
        "2: use tokenizer to decoder and\n"
      ],
      "metadata": {
        "id": "-2oXzuYaVj1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "eval_tokenizer = tokenizer\n",
        "\n",
        "def gen(model,p, maxlen=200, sample=True):\n",
        "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
        "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen,do_sample=sample,num_return_sequences=1,temperature=0.5,num_beams=1,top_p=0.99,\n",
        "    ).to('cpu') #multi options to generate: beam search, top k, top p. We choose top p here.##generate expects named tensors (input_ids, attention_mask, …) on the same device as the model.\n",
        "    #Using .to(\"cuda\") + ** cleanly moves and passes them.\n",
        "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)\n",
        "\n",
        "\n",
        "from transformers import set_seed\n",
        "seed = 100\n",
        "set_seed(seed) #fix the random choosing happened in gen() below\n",
        "index = 1\n",
        "prompt = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput: {prompt}\\nOutput:\\n\"\n",
        "res = gen(original_model,formatted_prompt,100,)\n",
        "dash_line = '-'.join('' for x in range(200))\n",
        "print(res[0])\n",
        "print(dash_line)\n",
        "output = res[0].split('Output:\\n')[1]\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - 0-SHOT:\\n{output}') # My feedback after reading the results: baseline is true summary, and 0-shot is more like keep writing the story from prompt.\n",
        "\n",
        "base_summary = []\n",
        "for i in range(0,10):\n",
        "  index = i\n",
        "  prompt = dataset['test'][index]['dialogue']\n",
        "  summary = dataset['test'][index]['summary']\n",
        "  formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput: {prompt}\\nOutput:\\n\"\n",
        "  #pprint(formatted_prompt)\n",
        "  res = gen(original_model,formatted_prompt,100,)\n",
        "  dash_line = '-'.join('' for x in range(100))\n",
        "  output = res[0].split('Output:\\n')[1]\n",
        "  base_summary.append(output)\n",
        "\n",
        "pprint(base_summary[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e72a1vgkVkCT",
        "outputId": "5dff1519-9a0e-482f-c2a2-dfee2b234c45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruct: Summarize the following conversation.\n",
            "Input: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Output:\n",
            "The conversation is about a company policy restricting the use of Instant Messaging during work hours. The policy applies to all communications, internal and external, and any employee found using Instant Messaging will receive a warning and be placed on probation for a second offense. The memo is to be distributed to all employees by 4 pm. The policy aims to minimize distractions and improve productivity. The conversation involves a company manager, Ms. Dawson, dictating a memo to an assistant. The assistant asks clarifying questions to ensure\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "Instruct: Summarize the following conversation.\n",
            "Input: #Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Output:\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - 0-SHOT:\n",
            "The conversation is about a company policy restricting the use of Instant Messaging during work hours. The policy applies to all communications, internal and external, and any employee found using Instant Messaging will receive a warning and be placed on probation for a second offense. The memo is to be distributed to all employees by 4 pm. The policy aims to minimize distractions and improve productivity. The conversation involves a company manager, Ms. Dawson, dictating a memo to an assistant. The assistant asks clarifying questions to ensure\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('A company-wide memo restricting all office communications to email '\n",
            " 'correspondence and official memos, with a warning and potential termination '\n",
            " 'for employees who violate this policy. The memo is to be distributed by 4 '\n",
            " 'pm. External communications are also restricted, and employees who use '\n",
            " 'Instant Messaging to communicate with clients will have to change their '\n",
            " 'methods. Department heads will handle any questions regarding this new '\n",
            " 'policy. The memo is to be taken as an intra-office memo. The company is '\n",
            " 'taking this action to prevent time-wasting. The memo')\n",
            "CPU times: user 1min 16s, sys: 153 ms, total: 1min 16s\n",
            "Wall time: 1min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main stage:\n",
        "\n",
        "We want to finetune base model now.\n",
        "\n",
        "The first step is data preprocess. We already created new dataset[train]['text'], which combines input:\\n text + output:\\n summary together.\n",
        "Now we use tokenizer we setup earlier, and transfer text data to token ids. (embedding from tokens to vectors is inside llama transformer model, so we don't need to do this step)\n",
        "\n",
        "It's good data for model to learn now.\n",
        "\n",
        "Next few cells is for data preprocess."
      ],
      "metadata": {
        "id": "Att8-kScAkPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\",\n",
        "                           \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Tokenizing a batch\n",
        "    \"\"\"\n",
        "    return tokenizer(batch[\"text\"],max_length=max_length,truncation=True,) # truncation(means cut) and padding(means add) generate tokens of each text paragraph with same length."
      ],
      "metadata": {
        "id": "5G5dY1nHAlOf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed,\n",
        "dataset):\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#function in .map() acts on dataset sample by sample\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length,\n",
        "    tokenizer=tokenizer)#fixed input max—length and tokenizer with preprocess—batch function because too many input，I may make mistake later\n",
        "    dataset = dataset.map(\n",
        "    _preprocessing_function,\n",
        "    batched=True,\n",
        "    remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
        "    )#function in .map() acts on dataset sample by sample\n",
        "\n",
        "    # filter data to drop sample that longer than max_length,\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) <=\n",
        "    max_length)\n",
        "    # Shuffle data with fixed seed to keep result fixed\n",
        "    dataset = dataset.shuffle(seed=seed)\n",
        "    print(\"Preprocessing dataset done.\")\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "1P3H4mEN1GjR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply those tools and setups above to our dataset\n",
        "max_length = get_max_length(original_model)\n",
        "print(max_length)\n",
        "train_dataset = preprocess_dataset(tokenizer, max_length,seed,\n",
        "dataset['train'])\n",
        "eval_dataset = preprocess_dataset(tokenizer, max_length,seed,\n",
        "dataset['validation']) #validation dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "df0db10f69454a16aa42e75d33889925",
            "89772fffda604ef8affcc6828fac60be",
            "975d1d759b734b7ea8f0bd4302eecb43",
            "3bd8c6ae39c746909bea9da99f8f6c11",
            "5fcb49d961ee4861801739988992e093",
            "9ed9ef5043d7480a9f4a875d55ad1bca",
            "63ddd9260b594edfaa71ff522efada09",
            "5f8bc4a25c4c4ee98d9e06c062930a5e",
            "1b65472c410f4c40a891886960d15fb9",
            "3d9b9e14f1ef479c9d3fb1c7d26856fd",
            "f215345316824ba19b4650fb08194557",
            "0238534a1f90498cba9a4ae1faf706f2",
            "aec42b4c030845f1b9a55a525353c927",
            "8b54aab0a0dd4f959c632987124a1f68",
            "52b2c49314ee4f6c8c126b67c5f2244d",
            "d5d6c85742b441269b775c6fdb7f46c3",
            "31ed349d888b4be49bc7a991142939f7",
            "93168488c84246c3ba669c235951a141",
            "b9bbc068f2f14f28a5c9fdb042401794",
            "25f46aa1ed7b49cdb525be4220d887e6",
            "5381570c1fd348b29dced283fe79ba6d",
            "4563f568bb784b59a42103e36c035b7f"
          ]
        },
        "id": "hq5WFwNqEMLF",
        "outputId": "0b4ef446-fe93-48b0-cf73-81e659c27c83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 8192\n",
            "8192\n",
            "Preprocessing dataset...\n",
            "Preprocessing dataset done.\n",
            "Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df0db10f69454a16aa42e75d33889925"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0238534a1f90498cba9a4ae1faf706f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In second step, we build a finetune model. There are two parts: first part is setup of Lora and second is finetune model. Notice that we basicly just work on parameters of LoraConfig and transformers.trainer. We don't need to build these two moudle, but it's necessary to know the concepts and structure of these two."
      ],
      "metadata": {
        "id": "ewcCQNqVcNhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "config = LoraConfig(\n",
        "r=16, #Rank I use T4 GPU, so r=32 is too 'OOM' for me.\n",
        "lora_alpha=16,\n",
        "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "bias=\"none\",\n",
        "lora_dropout=0.01, # Conventional\n",
        "task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "original_model.gradient_checkpointing_enable()\n",
        "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "original_model = prepare_model_for_kbit_training(original_model)\n",
        "peft_model = get_peft_model(original_model, config)"
      ],
      "metadata": {
        "id": "NmWg0uKu1Gs4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/peft-dialogue-summary-training/final-checkpoint'\n",
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "output_dir = output_dir,\n",
        "warmup_steps=1,\n",
        "per_device_train_batch_size=1,\n",
        "gradient_accumulation_steps=1,\n",
        "max_steps=512, # 'OOM'problem, I have to reduce maz_steps\n",
        "learning_rate=2e-4,\n",
        "optim=\"paged_adamw_8bit\",\n",
        "logging_steps=100,\n",
        "logging_dir=\"/content/logs\",\n",
        "save_strategy=\"steps\",\n",
        "save_steps=100,\n",
        "eval_strategy=\"steps\", #should replace evaluation_strategy=\"steps\" with eval_strategy=\"steps\". The argument name was changed in newer versions of the transformers library.\n",
        "eval_steps=100,\n",
        "do_eval=True,\n",
        "gradient_checkpointing=True,\n",
        "report_to=\"none\",\n",
        "overwrite_output_dir = True,\n",
        "group_by_length=True,\n",
        "#save_total_limit=2,             # 控制checkpoint数量，省空间\n",
        ")\n",
        "\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "# Explicitly move the model to the GPU before initializing the trainer\n",
        "#peft_model.to('cuda')#为什么不要 peft_model.to('cuda')\n",
        "\n",
        "#对 4bit/8bit（bitsandbytes）+ device_map=\"auto\" 的模型，手动 .to('cuda') 容易让部分量化层仍留在 CPU，导致 Accelerate 在比较设备时拿到 None（CPU 的 index），从而触发 torch.device(None) 的报错。\n",
        "#让 from_pretrained(device_map=\"auto\") 和 Trainer/Accelerate 统一管理设备放置最稳妥。\n",
        "\n",
        "\n",
        "peft_trainer = transformers.Trainer(\n",
        "model=peft_model,\n",
        "train_dataset=train_dataset,\n",
        "eval_dataset=eval_dataset,\n",
        "args=peft_training_args,\n",
        "data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,\n",
        "mlm=False),\n",
        ")"
      ],
      "metadata": {
        "id": "1-7rjBRpeCrw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train peft model, and check how many parameters trained in training process.\n",
        "\n",
        "Free memory after training because we only care trained new model."
      ],
      "metadata": {
        "id": "o1zEGvo4_teo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "# OutOfMemoryError OOM 多半是“序列太长 + CE上 fp32 中间张量占内存”。首选降低 max_length，并开启 fp16/bf16。\n",
        "#训练先关评估，能跑通后再恢复。\n",
        "#不手动 .to(\"cuda\")，把设备交给 device_map=\"auto\"。\n",
        "#必要时把 LoRA rank 降到 8/16。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "VqnXJlBQ_tqX",
        "outputId": "484ffa7b-f6de-49b4-90c6-f5112412fe27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='512' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [512/512 06:34, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.476700</td>\n",
              "      <td>1.435606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.360400</td>\n",
              "      <td>1.420978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.349200</td>\n",
              "      <td>1.412585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.364000</td>\n",
              "      <td>1.405970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.330000</td>\n",
              "      <td>1.401833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=512, training_loss=1.3809343315660954, metrics={'train_runtime': 395.6839, 'train_samples_per_second': 1.294, 'train_steps_per_second': 1.294, 'total_flos': 5977928817254400.0, 'train_loss': 1.3809343315660954, 'epoch': 0.256128064032016})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc648513"
      },
      "source": [
        "Since we encountered issues with the standard `Trainer`, let's try using the `SFTTrainer` from the `trl` library, which is designed for supervised fine-tuning tasks like this and might handle the integration with QLoRA and BitsAndBytes more smoothly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a43e0ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7bf056db-5169-4d6c-c944-9ff16a753821"
      },
      "source": [
        "'''This is a backup training plan if the standard Trainer above fails.\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Reusing the previously defined model, tokenizer, datasets, and training arguments\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=config, # Use the LoRA config here\n",
        "    dataset_text_field=\"text\", # Specify the column containing the formatted text\n",
        "    max_seq_length=max_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=peft_training_args,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# Start training with the SFTTrainer\n",
        "sft_trainer.train()'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a backup training plan if the standard Trainer above fails.\\nfrom trl import SFTTrainer\\n\\n# Reusing the previously defined model, tokenizer, datasets, and training arguments\\n\\nsft_trainer = SFTTrainer(\\n    model=peft_model,\\n    train_dataset=train_dataset,\\n    eval_dataset=eval_dataset,\\n    peft_config=config, # Use the LoRA config here\\n    dataset_text_field=\"text\", # Specify the column containing the formatted text\\n    max_seq_length=max_length,\\n    tokenizer=tokenizer,\\n    args=peft_training_args,\\n    packing=False,\\n)\\n\\n# Start training with the SFTTrainer\\nsft_trainer.train()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65660a08"
      },
      "source": [
        "After training, you can evaluate the model and save the trained adapter weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\n \\\n",
        "all model parameters: {all_model_params}\\n \\\n",
        "percentage of trainable model parameters: \\\n",
        "{100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "# original_model is deleted before this function is called, so this will raise an error.\n",
        "# I'll print the trainable parameters before deleting the original model.\n",
        "print_number_of_trainable_model_parameters(peft_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MwfAAsysBAA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "273e1dca-b64c-4194-f2bf-c167e061afe9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trainable model parameters: 13631488\\n all model parameters: 4554231808\\n percentage of trainable model parameters: 0.30%'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory（optional，I use colab pro and choose 80G ram, which is more than enough）\n",
        "'''del original_model # original_model is not defined in this cell, so this will raise an error.\n",
        "del peft_trainer\n",
        "torch.cuda.empty_cache()'''"
      ],
      "metadata": {
        "id": "kG9qpmgFBygc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28a0f30c-5a2b-4559-e126-64342f89e390"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'del original_model # original_model is not defined in this cell, so this will raise an error.\\ndel peft_trainer\\ntorch.cuda.empty_cache()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now added trained Lora parameters \\delta W to base model parameters W, and check by generating summary."
      ],
      "metadata": {
        "id": "7GfJ3ezVGw4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import base model again if uncomment Free memory step.\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "quant_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=compute_dtype,\n",
        "bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "model_path,\n",
        "dtype=compute_dtype,#`torch_dtype` is deprecated! Use `dtype` instead!\n",
        "device_map={\"\": 0},\n",
        "quantization_config=quant_config\n",
        ")"
      ],
      "metadata": {
        "id": "e-g0bemrDazL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7744a51826824d62b356bb6925fe5a9f",
            "b0a64a28516c446eb12ab3dbc7b3401c",
            "fb6e7952fe0245e7b6328a1cf45ff8b1",
            "1aaed0ac728248e68d313abcbf469a20",
            "2c92f38ecaa545ba9d4ec1a18810e9e4",
            "fe6d735f25484e7e89c3f84957d4a131",
            "7312e177d462458da32297e589ac02a3",
            "beb025d6e45448a997532bffe30cb07c",
            "9e6f76f3a38744be92840f482da201af",
            "f2edb33ae9ae4ad08f21f72e77ba47f9",
            "339fc358e46c41a3ab68e24b8eb44881"
          ]
        },
        "outputId": "be12690f-6f9f-4e15-94eb-134f2d975fed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7744a51826824d62b356bb6925fe5a9f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import tokenizer again if uncomment Free memory step.\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(model_path, add_bos_token=True,\n",
        "trust_remote_code=True, use_fast=False)\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "c3rAmskxDmmG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge parameters from Lora and base model\n",
        "from peft import PeftModel\n",
        "ft_model = PeftModel.from_pretrained(\n",
        "base_model,\n",
        "\"/content/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",\n",
        "torch_dtype=torch.float16,\n",
        "is_trainable=False\n",
        ") #作用：把“已经训练好的 LoRA 适配器权重”从磁盘（或Hub）加载到一个基座模型上，返回一个可用的 PeftModel。\n",
        "#注意：from_pretrained 是“挂载适配器”，并没有把 ΔW 合并进基座权重。若你想得到“已融合”的单一模型，需要再调用 merge_and_unload()"
      ],
      "metadata": {
        "id": "JH6lA-HeDtd9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "index = 0\n",
        "prompt = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput:{prompt}\\nOutput:\\n\"\n",
        "res = gen(ft_model,formatted_prompt,100,)\n",
        "#print(res[0])\n",
        "output = res[0].split('Output:\\n')[1]\n",
        "dash_line = '-'.join('' for x in range(200))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL GENERATION:\\n{output}')\n",
        "\n",
        "# It's clear to see that fine-tuned ft_model has a summary close to human baseline summary from dataset.\n",
        "# Compared with base model, ft_model has slightly better ability of dialog summary!\n",
        "# I add 'scores' to evaluate how close the generated summary is to human baseline summary. Please check it on github"
      ],
      "metadata": {
        "id": "Qtc2X_TwDw-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d4d68d-b144-4dc0-8f23-ccdd016c1ed1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "Instruct: Summarize the following conversation.\n",
            "Input:#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Output:\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PEFT MODEL GENERATION:\n",
            "Ms. Dawson needs to take a dictation from #Person1#, who tells her that all office communications are restricted to email and official memos, and Instant Messaging is strictly prohibited. #Person2# asks about the scope of the policy and #Person1# says it applies to internal and external communications. #Person2# types up the memo and will distribute it to all employees before 4 pm. #Person1# warns employees that Instant Messaging is wasting time. #Person1# will\n",
            "CPU times: user 10.6 s, sys: 6.06 ms, total: 10.6 s\n",
            "Wall time: 10.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, based on ref summary from original dataset, we use Rouge score from HuggingFace to evaluate RougeL scores of base Llama 3 model and our finetuned model. We expect an improvment of scores."
      ],
      "metadata": {
        "id": "NDQCl9T96wUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare summary dataset from ref, Llama, finetuned model\n",
        "ref_origin = []\n",
        "for i in range(0,10):\n",
        "  ref_origin.append(dataset['test'][i]['summary'])\n",
        "\n",
        "pprint(ref_origin[0])\n",
        "\n",
        "ft_summary = []\n",
        "for i in range(0,10):\n",
        "  index = i\n",
        "  prompt = dataset['test'][index]['dialogue']\n",
        "  summary = dataset['test'][index]['summary']\n",
        "  formatted_prompt = f\"Instruct: Summarize the following conversation.\\nInput: {prompt}\\nOutput:\\n\"\n",
        "  #pprint(formatted_prompt)\n",
        "  res = gen(ft_model,formatted_prompt,100,)\n",
        "  dash_line = '-'.join('' for x in range(100))\n",
        "  output = res[0].split('Output:\\n')[1]\n",
        "  ft_summary.append(output)\n",
        "\n",
        "print(dash_line)\n",
        "pprint(f'MODEL GENERATION - FEW-SHOT:\\n{ft_summary[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgH9h6BX7TXL",
        "outputId": "c5e68286-0a74-4cde-b0f6-4188a26eab62"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Ms. Dawson helps #Person1# to write a memo to inform every employee that '\n",
            " 'they have to change the communication method and should not use Instant '\n",
            " 'Messaging anymore.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "('MODEL GENERATION - FEW-SHOT:\\n'\n",
            " 'Ms. Dawson takes a dictation from #Person1# to restrict all office '\n",
            " 'communications to email correspondence and official memos. #Person1# tells '\n",
            " 'her the policy will apply to all communications, including internal and '\n",
            " 'external ones. #Person1# also warns that employees who persist in using '\n",
            " 'Instant Messaging will face termination. Ms. Dawson will get the memo typed '\n",
            " 'up and distributed to all employees before 4 pm. #Person1# asks her to '\n",
            " 'continue with the memo. Ms. Dawson writes down what')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#现象\n",
        "\n",
        "问题：\n",
        "\n",
        "第一次加载 original_model（刚从 from_pretrained 导入时） → 生成正常\n",
        "\n",
        "训练完 PEFT（QLoRA）模型后\n",
        "\n",
        "ft_model → 生成答案更好\n",
        "\n",
        "同样的 original_model 再次生成 → 只输出 “Ms\\n\\n\\n\\n…”（退化输出）\n",
        "\n",
        "回答 from GPT5:\n",
        "\n",
        "🎯 最终原因：你的 original_model 被训练流程“破坏”了（状态被改写）\n",
        "\n",
        "我已经完整阅读并逐行检查你整个训练流程。你在训练 QLoRA 的时候直接在 original_model 上注入了 LoRA 层、修改了模型 config、启用了 gradient checkpointing、禁用了 cache，并在 prepare_model_for_kbit_training() 里改变了模型 forward 行为。\n",
        "\n",
        "🔥🔥 所以 original_model 已经不再是原始模型了！\n",
        "\n",
        "你训练完之后：\n",
        "\n",
        "original_model = prepare_model_for_kbit_training(original_model)\n",
        "peft_model = get_peft_model(original_model, config)\n",
        "\n",
        "\n",
        "这段操作直接在 原模型内部做了以下更改（这些都是 peft + QLoRA 会干的）：\n",
        "\n",
        "📌 1. 你把 LoRA adapter 注入了 original_model 自己\n",
        "\n",
        "PEFT 的实现方式是：\n",
        "\n",
        "把 LoRA module 注入到 base model 的 Q/K/V/O projection 层\n",
        "\n",
        "替换了这些 Linear 层的 forward 函数\n",
        "\n",
        "修改了 model.config 里的 use_cache、gradient_checkpointing、training flags\n",
        "\n",
        "所以：\n",
        "\n",
        "original_model 在你训练完之后已经不是原来的 8B 模型，它已经是“QLoRA 训练准备状态下的模型”，forward 路径不一样。\n",
        "\n",
        "📌 2. 你对 original_model 开启了 gradient checkpointing\n",
        "\n",
        "来自 PDF：\n",
        "\n",
        "original_model.gradient_checkpointing_enable()\n",
        "\n",
        "\n",
        "这会改变模型前向传播的行为，尤其会影响：\n",
        "\n",
        "KV Cache\n",
        "\n",
        "hidden states 顺序\n",
        "\n",
        "dropout\n",
        "\n",
        "training flag\n",
        "\n",
        "对于推理来说，这是不合法状态。\n",
        "\n",
        "于是 original_model 在推理阶段会输出垃圾，例如：\n",
        "\n",
        "Ms\n",
        "\\n\\n\\n\\n...\n",
        "\n",
        "📌 3. prepare_model_for_kbit_training() 直接修改了模型内部\n",
        "\n",
        "该函数会：\n",
        "\n",
        "将 layernorm 权重转成 fp32\n",
        "\n",
        "修改 model.forward，让某些层返回新的结构\n",
        "\n",
        "将 embedding / lm_head cast 成不同 dtype\n",
        "\n",
        "注入 zero_grad hooks\n",
        "\n",
        "禁用/修改缓存行为（特别是 use_cache=False 时）\n",
        "\n",
        "它直接改变模型行为。\n",
        "\n",
        "📌 4. 你在训练前把 original_model.config.use_cache 改成 False\n",
        "\n",
        "PDF 原文：\n",
        "\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "\n",
        "由于 peft_model 是基于 original_model 构建的，\n",
        "这实际上把 original_model.config.use_cache 也改成 False。\n",
        "\n",
        "Llama 在推理时 use_cache=False 会导致重复生成换行、状态错乱、退化输出。\n",
        "\n",
        "这也是你看到：\n",
        "\n",
        "Ms\n",
        "\\n\\n\\n\\n...\n",
        "\n",
        "\n",
        "的直接原因之一。\n",
        "\n",
        "📌 5. 你在训练过程中修改了 original_model 对象本身\n",
        "\n",
        "关键问题：\n",
        "你没有在训练前 clone 一份模型。\n",
        "\n",
        "训练流程里用的是：\n",
        "\n",
        "original_model  # <-- 训练前的模型\n",
        "prepare_model_for_kbit_training(original_model)\n",
        "peft_model = get_peft_model(original_model)\n",
        "\n",
        "\n",
        "也就是说：\n",
        "\n",
        "training 是在 original_model 上“原地”修改的。\n",
        "\n",
        "训练后的 original_model = 被改写、注入 LoRA、禁用 cache、修改 forward、开启 checkpointing 的模型\n",
        "\n",
        "它不能再做正常推理。\n",
        "\n",
        "📌 6. 你训练后 “重建 base_model” 的方式是正确的，但你没有重建 original_model\n",
        "\n",
        "你训练完之后做了：\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(...)\n",
        "\n",
        "\n",
        "这是正确的。\n",
        "\n",
        "但是你继续用 old original_model（被训练破坏过的模型）来做推理，就会输出垃圾。\n",
        "\n",
        "🧨 所以最终现象就产生了：\n",
        "模型\t状态\t输出\n",
        "original_model（第一次加载）\t干净\t正常 summary\n",
        "original_model（训练后）\t已被 QLoRA 修改，forward 改变，cache 禁用\tMs + 大量空行（退化输出）\n",
        "ft_model\t正常加载（合并 LoRA）\t输出良好\n",
        "🎯 解决方案（最推荐）\n",
        "✔ 解决方案 1：训练后重新加载 original_model\n",
        "\n",
        "不要再使用被训练过的 original_model。\n",
        "\n",
        "训练后：\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "quant_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=compute_dtype,\n",
        "  bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    dtype=compute_dtype,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")\n",
        "\n",
        "\n",
        "然后 original_model 会再次恢复正常输出。\n",
        "\n",
        "✔ 解决方案 2：训练前 clone 模型\n",
        "import copy\n",
        "base_model = copy.deepcopy(original_model)\n",
        "\n",
        "\n",
        "让 original_model 保持未被污染。\n",
        "\n",
        "✔ 解决方案 3（次优）\n",
        "\n",
        "把你推理 prompt 改为 Llama3 官方 ChatML，避免 Ms \\n\\n\\n\\n\n",
        "\n",
        "🎯 最终一句话总结\n",
        "\n",
        "**因为你在训练 LoRA 的过程中修改了 original_model（注入 LoRA、改变 forward、禁用 use_cache、开启 checkpointing），导致 original_model 在训练后不能再正常推理，输出变成 “Ms + 一堆换行”。\n",
        "\n",
        "重新加载 original_model 就恢复正常。**"
      ],
      "metadata": {
        "id": "konP3Kel6BaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "c2GgUklW4S4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute RougeL scores of finetuned and Llama model. Check the differnece\n",
        "\n",
        "\n",
        "# 加载 ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# 计算整体 ROUGE 分数\n",
        "scores_ft = rouge.compute(\n",
        "    predictions=ft_summary,\n",
        "    references=ref_origin,\n",
        "    # 可选参数：\n",
        "    use_stemmer=True  # 英文可以开，对中文一般无意义\n",
        ")\n",
        "\n",
        "print(scores_ft)\n",
        "\n",
        "scores_noft = rouge.compute(\n",
        "    predictions=base_summary,\n",
        "    references=ref_origin,\n",
        "    # 可选参数：\n",
        "    use_stemmer=True  # 英文可以开，对中文一般无意义\n",
        ")\n",
        "\n",
        "print(scores_noft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "b0372b696f0d442f8bd0f31bc77560ae",
            "be6c284be8fd42e987399f47f7707417",
            "a9b5410a0bc84976800841ceaf542228",
            "2eb38f7d0cbe431faf02f5a0ae625226",
            "5915c28b56c94a17b8328339dc781d14",
            "b47eac59527d43b88ce2ab3e0e276474",
            "6267339145b244d0bb1352462acae3e8",
            "01d4b3f2dba94ccda905eb6034eff490",
            "aa5547f4e68f45668808ebfa032e9ca0",
            "1fd9f8e584304415a3f1b8656af87895",
            "6f73191bcf844f9d84f0739d9feb7d8d"
          ]
        },
        "id": "OT4IB59Z7pWo",
        "outputId": "84a664a5-4f5d-42ab-b72a-4c007b4956ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5ef24849223a1c3399cc2424196ed4412f713c80712da8b1e8a6cac8dffcf352\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0372b696f0d442f8bd0f31bc77560ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': np.float64(0.32102673459778264), 'rouge2': np.float64(0.14050694774228806), 'rougeL': np.float64(0.23785597432305772), 'rougeLsum': np.float64(0.23747036434151342)}\n",
            "{'rouge1': np.float64(0.25983521043970326), 'rouge2': np.float64(0.08840670723749743), 'rougeL': np.float64(0.1785822714906734), 'rougeLsum': np.float64(0.17853018443207122)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "\n",
        "ft_model{'rouge1': np.float64(0.32102673459778264), 'rouge2': np.float64(0.14050694774228806), 'rougeL': np.float64(0.23785597432305772), 'rougeLsum': np.float64(0.23747036434151342)}\n",
        "\n",
        "base_model{'rouge1': np.float64(0.25983521043970326), 'rouge2': np.float64(0.08840670723749743), 'rougeL': np.float64(0.1785822714906734), 'rougeLsum': np.float64(0.17853018443207122)}\n",
        "\n",
        "Therefore, ft_model > base_model, has 30% better ability for summerizing."
      ],
      "metadata": {
        "id": "A-J6wjeBBlWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple induction of Rouge Score\n",
        "\n",
        "ROUGE = Recall-Oriented Understudy for Gisting Evaluation\n",
        "是自动摘要任务中最经典的评估指标。它本质上测量：\n",
        "模型生成的 summary 中，有多少内容和参考摘要（人工写的）重叠？\n",
        "\n",
        "| 指标             | 关注点                            |\n",
        "| -------------- | ------------------------------ |\n",
        "| **ROUGE-1**    | unigram（1 元词/字）重叠              |\n",
        "| **ROUGE-2**    | bigram（2 元连续词）重叠               |\n",
        "| **ROUGE-L**    | 最长公共子序列（LCS），关注顺序一致性           |\n",
        "| **ROUGE-Lsum** | sentence-wise LCS（专门优化新闻/摘要任务） |\n",
        "\n",
        "定义\n",
        "\n",
        "# ROUGE 指标公式汇总（ROUGE-1 / ROUGE-2 / ROUGE-L / ROUGE-Lsum）\n",
        "\n",
        "下面设：\n",
        "\n",
        "- 参考摘要（reference）记为序列  \n",
        "  \\( R = (r_1, r_2, \\dots, r_m) \\)\n",
        "- 模型摘要（prediction）记为序列  \n",
        "  \\( P = (p_1, p_2, \\dots, p_n) \\)\n",
        "\n",
        "这里的 \\(r_i, p_j\\) 可以理解为 **英文的词（token）**。\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ ROUGE-1（unigram 重叠）\n",
        "\n",
        "ROUGE-1 衡量的是 **unigram（单个词）重叠率**，一般用召回率（recall）形式。\n",
        "\n",
        "定义词的 multiset（多重集合）：\n",
        "\n",
        "- \\( U(R) \\)：参考摘要的所有 unigram 及其出现次数  \n",
        "- \\( U(P) \\)：预测摘要的所有 unigram 及其出现次数  \n",
        "\n",
        "那么所有 unigram 的重叠数为：\n",
        "\n",
        "\\[\n",
        "\\text{Overlap}_1 = \\sum_{w \\in V} \\min\\big( \\text{count}_R(w), \\ \\text{count}_P(w) \\big)\n",
        "\\]\n",
        "\n",
        "其中 \\(V\\) 是所有可能词的集合。\n",
        "\n",
        "**ROUGE-1（召回）** 定义为：\n",
        "\n",
        "\\[\n",
        "\\text{ROUGE-1} =\n",
        "\\frac{\\text{Overlap}_1}{\\sum_{w \\in V} \\text{count}_R(w)}\n",
        "= \\frac{\\text{Overlap}_1}{|R|}\n",
        "\\]\n",
        "\n",
        "即：  \n",
        "> 预测摘要中命中参考摘要的 unigram 数量 ÷ 参考摘要总 unigram 数量。\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ ROUGE-2（bigram 重叠）\n",
        "\n",
        "ROUGE-2 衡量的是 **bigram（连续两词）重叠率**。  \n",
        "先定义 bigram 序列：\n",
        "\n",
        "- 参考摘要的 bigram 序列：  \n",
        "  \\[\n",
        "  B(R) = \\big\\{(r_1,r_2), (r_2,r_3), \\dots, (r_{m-1},r_m)\\big\\}\n",
        "  \\]\n",
        "- 预测摘要的 bigram 序列：  \n",
        "  \\[\n",
        "  B(P) = \\big\\{(p_1,p_2), (p_2,p_3), \\dots, (p_{n-1},p_n)\\big\\}\n",
        "  \\]\n",
        "\n",
        "同样用 multiset 计数，用：\n",
        "\n",
        "\\[\n",
        "\\text{Overlap}_2 = \\sum_{b \\in \\mathcal{B}}\n",
        "\\min\\big( \\text{count}_{B(R)}(b), \\ \\text{count}_{B(P)}(b) \\big)\n",
        "\\]\n",
        "\n",
        "其中 \\(\\mathcal{B}\\) 是所有可能 bigram 的集合。\n",
        "\n",
        "**ROUGE-2（召回）** 为：\n",
        "\n",
        "\\[\n",
        "\\text{ROUGE-2} =\n",
        "\\frac{\\text{Overlap}_2}{|B(R)|}\n",
        "=\n",
        "\\frac{\\text{Overlap}_2}{m-1}\n",
        "\\]\n",
        "\n",
        "即：  \n",
        "> 预测摘要中命中参考摘要的 bigram 数量 ÷ 参考摘要总 bigram 数量。\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ ROUGE-L（基于最长公共子序列 LCS）\n",
        "\n",
        "ROUGE-L 使用 **Longest Common Subsequence（LCS，最长公共子序列）** 衡量两段文本的 **顺序一致性**。\n",
        "\n",
        "给定序列 \\(R, P\\)，其 LCS 长度记为：\n",
        "\n",
        "\\[\n",
        "LCS(R, P)\n",
        "\\]\n",
        "\n",
        "LCS 是在保持顺序的前提下，找到两个序列中最长的“相同子序列”，不要求连续。\n",
        "\n",
        "然后定义基于 LCS 的召回和精度：\n",
        "\n",
        "\\[\n",
        "R_L = \\frac{LCS(R,P)}{|R|}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "P_L = \\frac{LCS(R,P)}{|P|}\n",
        "\\]\n",
        "\n",
        "再定义 F1（或 F\\(_\\beta\\)）：\n",
        "\n",
        "\\[\n",
        "F_L =\n",
        "\\frac{(1+\\beta^2)\\,R_L\\,P_L}{R_L + P_L}\n",
        "\\]\n",
        "\n",
        "在很多实现中，\\(\\beta = 1\\)，即普通 F1：\n",
        "\n",
        "\\[\n",
        "F_L = \\frac{2\\,R_L\\,P_L}{R_L + P_L}\n",
        "\\]\n",
        "\n",
        "**实践中常报告的 ROUGE-L 就是这个 \\(F_L\\)**。\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ ROUGE-Lsum（面向多句摘要的 ROUGE-L）\n",
        "\n",
        "ROUGE-L 对整段文本当作一个长序列算一次 LCS，适合 **短文本**。  \n",
        "但真实摘要（尤其是新闻、对话摘要）往往由多句组成，句子级结构也很重要。\n",
        "\n",
        "**ROUGE-Lsum** 的思想：\n",
        "\n",
        "> 把参考摘要和预测摘要都切成多个句子，  \n",
        "> 然后按“句子对”匹配 LCS，再对所有句子求一个整体的 LCS 召回 / 精度。\n",
        "\n",
        "### 4.1 句子级表示\n",
        "\n",
        "把参考摘要拆成 \\(m\\) 个句子：\n",
        "\n",
        "\\[\n",
        "R = \\{ s^R_1, s^R_2, \\dots, s^R_m \\}\n",
        "\\]\n",
        "\n",
        "把预测摘要拆成 \\(n\\) 个句子：\n",
        "\n",
        "\\[\n",
        "P = \\{ s^P_1, s^P_2, \\dots, s^P_n \\}\n",
        "\\]\n",
        "\n",
        "每个句子本身又是一个 token 序列。\n",
        "\n",
        "### 4.2 “一对多句子匹配”策略\n",
        "\n",
        "对参考摘要中的每个句子 \\(s^R_i\\)，  \n",
        "在预测摘要所有句子中，找到 **LCS 最长的那一个句子**：\n",
        "\n",
        "\\[\n",
        "LCS_i = \\max_{j} LCS\\big(s^R_i,\\ s^P_j\\big)\n",
        "\\]\n",
        "\n",
        "也就是：  \n",
        "> 对每个参考句子，只找一个最相似的预测句子，不强行一一配对所有句子。\n",
        "\n",
        "然后对所有参考句子，把这些 LCS 长度加起来：\n",
        "\n",
        "\\[\n",
        "\\text{LCS}_{\\text{sum}} = \\sum_{i=1}^{m} LCS_i\n",
        "\\]\n",
        "\n",
        "这就是“句子级 LCS 的总和”。\n",
        "\n",
        "### 4.3 ROUGE-Lsum 的召回和精度\n",
        "\n",
        "定义：\n",
        "\n",
        "- 参考摘要总 token 数量：  \n",
        "  \\[\n",
        "  |R| = \\sum_{i=1}^{m} \\big|s^R_i\\big|\n",
        "  \\]\n",
        "- 预测摘要总 token 数量：  \n",
        "  \\[\n",
        "  |P| = \\sum_{j=1}^{n} \\big|s^P_j\\big|\n",
        "  \\]\n",
        "\n",
        "则 ROUGE-Lsum 的 recall 和 precision 为：\n",
        "\n",
        "\\[\n",
        "R_{Lsum} = \\frac{\\text{LCS}_{\\text{sum}}}{|R|}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "P_{Lsum} = \\frac{\\text{LCS}_{\\text{sum}}}{|P|}\n",
        "\\]\n",
        "\n",
        "同样构造 F1：\n",
        "\n",
        "\\[\n",
        "F_{Lsum} =\n",
        "\\frac{(1+\\beta^2)\\,R_{Lsum}\\,P_{Lsum}}{R_{Lsum} + P_{Lsum}}\n",
        "\\]\n",
        "\n",
        "通常 \\(\\beta = 1\\) 时：\n",
        "\n",
        "\\[\n",
        "F_{Lsum} = \\frac{2\\,R_{Lsum}\\,P_{Lsum}}{R_{Lsum} + P_{Lsum}}\n",
        "\\]\n",
        "\n",
        "**实践中报告的 ROUGE-Lsum 通常就是这个 \\(F_{Lsum}\\)**。\n",
        "\n",
        "---\n",
        "\n",
        "## 4.4 为什么需要 ROUGE-Lsum？\n",
        "\n",
        "对摘要/新闻这类任务：\n",
        "\n",
        "- 一篇摘要常由多个句子组成  \n",
        "- 句子内部的顺序，以及句子和句子之间的结构都很重要  \n",
        "- 简单地把整篇拼成一个长序列算 LCS，可能会：\n",
        "  - 把跨句子的匹配算在一起，丢失句子边界的信息\n",
        "  - 让长句子完全主导得分\n",
        "\n",
        "ROUGE-Lsum 通过 **“句子级一对多匹配 + 汇总 LCS”** 的方式：\n",
        "\n",
        "- 更关注“参考摘要每个句子有没有被覆盖到”  \n",
        "- 更贴合人类对“摘要是否完整覆盖关键信息”的直觉  \n",
        "- 适合评估 multi-sentence summary（这也是 Hugging Face 默认给 `rougeLsum` 的原因）\n",
        "\n",
        "---\n",
        "\t​\n",
        "\n",
        "\n",
        "注意：\n",
        "Hugging Face 的 ROUGE 默认是 recall-based。\n"
      ],
      "metadata": {
        "id": "k1JHe2JeiEWv"
      }
    }
  ]
}